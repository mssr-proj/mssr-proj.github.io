<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Stable and Efficient Single-Rollout RL for Multimodal Reasoning">
  <meta property="og:title" content="MSSR"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">


  <title>MSSR</title>
  <link rel="icon" type="image/x-icon" href="static/images/robot/traj_s.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Stable and Efficient Single-Rollout RL for Multimodal Reasoning
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ruiiu.github.io" target="_blank">Rui Liu</a><sup>1,2</sup>, 
              <span class="author-block">
                <a href="https://sites.google.com/site/yudiandoris/" target="_blank">Dian Yu</a><sup>1</sup>, 
              <span class="author-block">
                <a href="https://www.kelei.site/" target="_blank">Lei Ke</a><sup>1</sup>, 
              <span class="author-block">
                <a href="https://liuhl2000.github.io/" target="_blank">Haohin Liu</a><sup>3</sup>, 
              </span>
              <span class="author-block">
                <a href="https://yujunzhou.github.io/" target="_blank">Yujun Zhou</a><sup>4</sup>, 
              </span>
              <span class="author-block">
                <a href="https://zhenwen-nlp.github.io/" target="_blank">Zhenwen Liang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=G3OMbFSm858C&hl=en" target="_blank">Haitao Mi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://tokekar.com/" target="_blank">Pratap Tokekar</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=tMY31_gAAAAJ&hl=en" target="_blank">Dong Yu</a><sup>1</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Tencent AI Lab, Bellevue<br></span>
                    <span class="author-block"><sup>2</sup> University of Maryland, College Park<br></span>
                    <span class="author-block"><sup>3</sup> University of Virginia<br></span>
                    <span class="author-block"><sup>4</sup> University of Notre Dame<br></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2512.18215" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                      <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://www.arxiv.org/abs/2512.18215" target="https://www.arxiv.org/abs/2512.18215"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/proof.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Appendix</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
<!--                   <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/IMRL_App.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      <div class="content has-text-justified">
      <p>
        <b>Overview of the proposed IMRL approach for food acquisition</b>. 
        Given the last \(k\) steps of eye-in-hand RGB observations, the system segments the desired food using SAM,
        extracts features with an encoder (e.g., ResNet-50), and processes them through visual and physical representation modules to learn a joint representation \(z_{vp}\). 
        A temporal representation module produces \(z_u\) to capture dynamics, while the geometric representation module provides bowl fullness \(l\) and optimal scooping points \((x^∗, y^∗)\). 
        All these representations are integrated into a multi-dimensional representation \(z\), which, combined with robot proprioception, is used to generate robot actions through a control module.
       </p>
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). 
            However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. 
            While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, 
            often leading to training collapse. To address this training efficiency-stability trade-off, we introduce <b>MSSR</b> (Multimodal Stabilized Single-Rollout), 
            a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. 
            MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. 
            While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. 
            In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. 
            When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive 
            benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Experiments -->
<section class="approach">
  <div style="text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px; margin-top: 30px">
    Approach
  </div>

  <!-- Single Image Centered -->
  <div style="text-align: center; margin-bottom: 10px;">
    <img src="static/images/mssr.jpg" alt="Approach" style="width: 100%; max-width: 800px; height: auto; border-radius: 6px;">
  </div>

  <!-- Caption -->
  <div style="font-size: 16px; color: #555; max-width: 1000px; margin: 0 auto; text-align: justify; padding: 0 10px;">
    <p>
      <b>Overview of the proposed MSSR approach</b>. 
      Given a multimodal input, i.e., an image and the corresponding question, we generate a single rollout through the policy model. 
      We then use a Beta distribution to estimate the baseline value \( v \), compute the advantage \( A \), and normalize it across the batch. Finally, we propose entropy-based advantage shaping to preserve entropy and stabilize training.
    </p>
  </div>
</section>

<section class="experiments">
  <div style="text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px; margin-top: 30px">
    MSSR Performance Overview
  </div>

  <!-- 2x2 Image Grid -->
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 10px; margin-bottom: 15px;">

    <!-- Image 1 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/train_7b.jpg" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(a) Training accuracy</div>
    </div>

    <!-- Image 2 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/val_7b.jpg" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(b) Validation accuracy</div>
    </div>

    <!-- Image 3 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/radar_7b.png" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(c) Generalization performance</div>
    </div>
  </div>

  <!-- Caption -->
  <div style="font-size: 16px; color: #555; max-width: 1000px; margin: 0 auto; text-align: justify; padding: 0 10px;">
    <p>
      <b>Performance overview of MSSR</b>. 
      (a–b) Training and validation accuracy of MVSR (Multimodal Vanilla Single-Rollout), GRPO and our MSSR, trained on the Vision-R1-RL training set and validated on its corresponding validation set. 
      MSSR remains stable and improves steadily, whereas MVSR is unstable and collapses. Notably, MSSR reaches a similar final validation accuracy to GRPO with half of the training steps, 
      highlighting its superior training compute efficiency. (c) Our MSSR achieves higher generalization performance across diverse multimodal reasoning benchmarks, 
      including MathVerse, MathVista, MMK12, R1-Onevision-Bench, and HallusionBench, compared to other baselines including GRPO, RLOO, and REINFORCE++. 
      For fair comparisons, we have equivalent total number of rollouts per step for all methods.
    </p>
  </div>

  <div style="text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px; margin-top: 30px">
    Main Results
  </div>
  
<style>
    .results-table-container {
      width: 100%;
      max-width: 1000px;
      margin: 20px auto 0 auto; /* Reduced top margin from 30px to 20px */
      overflow-x: auto;
      font-family: inherit;
    }
    .table-caption {
      font-size: 16px;
      color: #555;
      margin-bottom: 10px; /* Reduced from 15px */
      text-align: justify;
      line-height: 1.5;
      padding: 0 10px;
    }
    .results-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 14px;
      text-align: center;
      color: #333;
    }
    .results-table th {
      border-top: 2px solid #333;
      border-bottom: 1px solid #333;
      padding: 6px 4px; /* Tightened padding */
      font-weight: bold;
    }
    .results-table td {
      padding: 4px 8px; /* Reduced vertical padding from 10px to 4px */
    }
    .results-table tr.section-header td {
      background-color: #f8f9fa;
      font-weight: 600;
      font-size: 12px; /* Slightly smaller font for headers */
      text-transform: uppercase;
      border-top: 1px solid #eee;
      border-bottom: 1px solid #eee;
      padding: 2px 8px; /* Very tight padding for section labels */
    }
    .results-table tr.group-divider td {
      border-top: 1px solid #333;
      padding-top: 6px; /* Small buffer for the start of a new group */
    }
    .results-table tr.bottom-border td {
      border-bottom: 2px solid #333;
    }
    .results-table .text-left {
      text-align: left;
    }
    .results-table .indent {
      padding-left: 25px;
    }
    .best {
      font-weight: bold;
    }
  </style>

  <div class="results-table-container">
    <div class="table-caption">
      <b>Table 1: Model generalization performance on diverse multimodal reasoning benchmarks.</b> 
      We compare MSSR with GRPO, RLOO, and REINFORCE++ baselines on Qwen2.5-VL 3B and 7B models. 
      MSSR outperforms other baselines, with Qwen2.5-VL-7B + MSSR achieving the strongest average performance across benchmarks.
    </div>

    <table class="results-table">
      <thead>
        <tr>
          <th class="text-left">Model</th>
          <th>MathVerse</th>
          <th>MathVista</th>
          <th>MMK12</th>
          <th>R1-Onevision Bench</th>
          <th>HallusionBench</th>
          <th>Avg.</th>
        </tr>
      </thead>
      <tbody>
        <tr class="section-header"><td colspan="7">SFT + RL</td></tr>
        <tr><td class="text-left">R1-Onevision-7B</td><td>46.0</td><td>62.9</td><td>43.5</td><td>35.2</td><td>67.2</td><td>51.0</td></tr>
        <tr><td class="text-left">OpenVLThinker-7B</td><td>45.8</td><td>70.0</td><td>53.5</td><td>34.7</td><td>60.0</td><td>52.8</td></tr>
        <tr><td class="text-left">VLAA-Thinker-7B</td><td>48.2</td><td>68.0</td><td>51.7</td><td>38.4</td><td>70.0</td><td>55.3</td></tr>
        
        <tr class="section-header"><td colspan="7">Zero RL</td></tr>
        <tr><td class="text-left">MM-Eureka-Qwen-7B</td><td>50.3</td><td>71.2</td><td>61.7</td><td>39.1</td><td>66.4</td><td>57.7</td></tr>
        <tr><td class="text-left">ThinkLite-VL-7B</td><td>47.3</td><td>71.9</td><td>57.6</td><td>35.7</td><td>70.9</td><td>56.7</td></tr>
        
        <tr class="group-divider">
          <td class="text-left">Qwen2.5-VL-3B</td><td>33.3</td><td>59.5</td><td>42.5</td><td>27.6</td><td>59.9</td><td>44.6</td>
        </tr>
        <tr><td class="text-left indent">+ GRPO</td><td>36.8</td><td>61.7</td><td>46.1</td><td class="best">30.2</td><td>62.3</td><td>47.4</td></tr>
        <tr><td class="text-left indent">+ RLOO</td><td>35.7</td><td>59.7</td><td>45.5</td><td>28.8</td><td>61.6</td><td>46.3</td></tr>
        <tr><td class="text-left indent">+ REINFORCE++</td><td>35.3</td><td>47.7</td><td>46.0</td><td>21.7</td><td>63.2</td><td>42.8</td></tr>
        <tr><td class="text-left indent">+ MSSR</td><td class="best">39.6</td><td class="best">63.0</td><td class="best">49.2</td><td>29.0</td><td class="best">66.6</td><td class="best">49.5</td></tr>
        
        <tr class="group-divider">
          <td class="text-left">Qwen2.5-VL-7B</td><td>45.8</td><td>67.2</td><td>48.1</td><td>34.6</td><td>68.4</td><td>52.8</td>
        </tr>
        <tr><td class="text-left indent">+ GRPO</td><td>48.5</td><td>70.0</td><td>55.8</td><td>37.7</td><td>69.7</td><td>56.3</td></tr>
        <tr><td class="text-left indent">+ RLOO</td><td>47.8</td><td>69.2</td><td>56.0</td><td>38.5</td><td>68.5</td><td>56.0</td></tr>
        <tr><td class="text-left indent">+ REINFORCE++</td><td>42.7</td><td>68.5</td><td>51.3</td><td>34.0</td><td>69.2</td><td>53.1</td></tr>
        <tr class="bottom-border">
          <td class="text-left indent">+ MSSR</td><td class="best">49.8</td><td class="best">71.1</td><td class="best">62.5</td><td class="best">39.2</td><td class="best">70.6</td><td class="best">58.6</td>
        </tr>
      </tbody>
    </table>
  </div>
  

  <div style="text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px; margin-top: 30px">
    Ablation Studies
  </div>

  <!-- 2x2 Image Grid -->
  <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 10px; margin-bottom: 15px;">

    <!-- Image 1 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/ablation_reward_accuracy_7b.png" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(a) Training accuracy</div>
    </div>

    <!-- Image 2 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/ablation_val_pass1_7b.png" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(b) Validation accuracy</div>
    </div>

    <!-- Image 3 -->
    <div style="width: 20%; text-align: center;">
      <img src="static/images/ablation_entropy_7b.png" alt="" style="width: 100%; height: 200px; object-fit: contain; border-radius: 6px;">
      <div style="font-size: 16px; color: #777; margin-top: 4px;">(c) Model entropy</div>
    </div>
  </div>

  <!-- Caption -->
  <div style="font-size: 16px; color: #555; max-width: 1000px; margin: 0 auto; text-align: justify; padding: 0 10px;">
    <p>
      <b>Ablation studies on effectiveness of techniques for preventing entropy collapse and stabilizing multimodal single-rollout training</b>. 
      <b>Cross-modal regularization</b>: This technique provides partial stabilization, increasing training accuracy but still resulting in degraded validation accuracy, 
      and both metrics remain below those achieved by MSSR. <b>Entropy loss</b>: Adding an entropy loss term partially preserves entropy and improves training accuracy toward the end of training,
      but validation performance still degrades and entropy is not maintained as effectively as in MSSR.
    </p>
  </div>
  
</section>

<section class="examples">
  <div style="text-align: center; font-size: 28px; font-weight: bold; margin-bottom: 10px; margin-top: 30px">
    Reasoning Output Comparison Examples
  </div>

  <!-- Single Image Centered -->
  <div style="text-align: center; margin-bottom: 10px;">
    <img src="static/images/example1.jpg" alt="" style="width: 100%; max-width: 800px; height: auto; border-radius: 6px;">
  </div>
  <div style="text-align: center; margin-bottom: 10px;">
    <img src="static/images/example2.png" alt="" style="width: 100%; max-width: 800px; height: auto; border-radius: 6px;">
  </div>
  <div style="text-align: center; margin-bottom: 10px;">
    <img src="static/images/example3.png" alt="" style="width: 100%; max-width: 800px; height: auto; border-radius: 6px;">
  </div>
  <div style="text-align: center; margin-bottom: 10px;">
    <img src="static/images/example4.png" alt="" style="width: 100%; max-width: 800px; height: auto; border-radius: 6px;">
  </div>

  <!-- Caption -->
  <div style="font-size: 16px; color: #555; max-width: 1000px; margin: 0 auto; text-align: justify; padding: 0 10px;">
    <p>
      <b>Comparison of reasoning outputs from GRPO and MSSR</b>. 
      While GRPO produce incorrect answers, MSSR successfully solves the problem, demonstrating its superior reasoning capability. 
      We highlight the critical reasoning steps that lead to GRPO's incorrect answer in <span style="color: red;">red</span>, 
      and the key steps enabling MSSR’s correct prediction in <span style="color: green;">green</span>.
    </p>
  </div>
</section>






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
            @article{liu2025stable,
              title={Stable and Efficient Single-Rollout RL for Multimodal Reasoning},
              author={Liu, Rui and Yu, Dian and Ke, Lei and Liu, Haolin and Zhou, Yujun and Liang, Zhenwen and Mi, Haitao and Tokekar, Pratap and Yu, Dong},
              journal={arXiv preprint arXiv:2512.18215},
              year={2025}
            }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
